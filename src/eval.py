"""
Evaluation metrics and plotting functions.
"""
import torch
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def evaluate_robustness(model, loader, attack_func, epsilon, device, **kwargs):
    """
    Evaluates model accuracy on adversarial examples generated by attack_func.
    
    Args:
        model: The trained model.
        loader: DataLoader for test data.
        attack_func: Function that generates adversarial examples (e.g., fgsm_attack).
        epsilon: Perturbation magnitude.
        device: CPU or GPU.
        **kwargs: Additional arguments for the attack function (e.g., alpha, num_iter).
    """
    model.eval()
    correct = 0
    total = 0
    
    # We don't use torch.no_grad() here because attacks need gradients!
    # However, we must ensure the model weights don't update.
    
    for inputs, labels in tqdm(loader, desc=f"Attacking (eps={epsilon})", leave=False):
        inputs, labels = inputs.to(device), labels.to(device)
        
        # Generate adversarial examples
        # Note: attack_func should handle gradient calculation internally
        perturbed_inputs = attack_func(model, inputs, labels, epsilon, **kwargs)
        
        # Re-classify the perturbed inputs
        with torch.no_grad():
            outputs = model(perturbed_inputs)
            _, predicted = outputs.max(1)
            
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
    acc = 100. * correct / total
    return acc

def plot_confusion_matrix(cm, classes, title='Confusion Matrix'):
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)
    plt.title(title)
    plt.ylabel('True Label')
    plt.xlabel('Predicted Label')
    plt.show()
